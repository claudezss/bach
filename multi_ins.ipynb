{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-16T01:19:13.481749Z",
     "start_time": "2025-03-16T01:19:13.458363Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pretty_midi\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Constants\n",
    "MAX_SEQ_LEN = 512  # Maximum sequence length\n",
    "VOCAB_SIZE = 512   # Size of vocabulary (pitch + duration + instrument + special tokens)\n",
    "D_MODEL = 256      # Embedding dimension\n",
    "N_HEADS = 8        # Number of attention heads\n",
    "N_LAYERS = 6       # Number of transformer layers\n",
    "D_FF = 1024        # Feedforward dimension\n",
    "DROPOUT = 0.1      # Dropout rate\n",
    "\n",
    "# Special tokens\n",
    "PAD_TOKEN = 0\n",
    "SOS_TOKEN = 1\n",
    "EOS_TOKEN = 2\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "class MusicTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, n_layers, d_ff, max_seq_len, dropout):\n",
    "        super(MusicTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_seq_len)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads,\n",
    "                                                  dim_feedforward=d_ff, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=n_layers)\n",
    "        decoder_layers = nn.TransformerDecoderLayer(d_model=d_model, nhead=n_heads,\n",
    "                                                  dim_feedforward=d_ff, dropout=dropout)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layers, num_layers=n_layers)\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        if src_mask is None:\n",
    "            src_mask = self.generate_square_subsequent_mask(src.size(0)).to(src.device)\n",
    "        if tgt_mask is None:\n",
    "            tgt_mask = self.generate_square_subsequent_mask(tgt.size(0)).to(tgt.device)\n",
    "\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "\n",
    "        tgt = self.embedding(tgt) * math.sqrt(self.d_model)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "\n",
    "        memory = self.transformer_encoder(src, src_mask)\n",
    "        output = self.transformer_decoder(tgt, memory, tgt_mask)\n",
    "        output = self.fc_out(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "class MIDIProcessor:\n",
    "    def __init__(self, vocab_size=VOCAB_SIZE):\n",
    "        # Reserved tokens\n",
    "        self.pad_token = PAD_TOKEN\n",
    "        self.sos_token = SOS_TOKEN\n",
    "        self.eos_token = EOS_TOKEN\n",
    "\n",
    "        # Token ranges\n",
    "        self.start_pitch = 10\n",
    "        self.num_pitches = 128\n",
    "        self.start_duration = self.start_pitch + self.num_pitches\n",
    "        self.num_durations = 100  # Quantized durations\n",
    "        self.start_instrument = self.start_duration + self.num_durations\n",
    "        self.num_instruments = 16  # General MIDI has 16 instrument families\n",
    "\n",
    "        assert self.start_instrument + self.num_instruments < vocab_size, \"Vocabulary size too small\"\n",
    "\n",
    "    def encode_note(self, pitch, duration_bin, instrument):\n",
    "        pitch_token = self.start_pitch + pitch\n",
    "        duration_token = self.start_duration + duration_bin\n",
    "        instrument_token = self.start_instrument + instrument\n",
    "        return [instrument_token, pitch_token, duration_token]\n",
    "\n",
    "    def decode_token(self, token):\n",
    "        if token < self.start_pitch:\n",
    "            return {\"type\": \"special\", \"value\": token}\n",
    "        elif token < self.start_duration:\n",
    "            return {\"type\": \"pitch\", \"value\": token - self.start_pitch}\n",
    "        elif token < self.start_instrument:\n",
    "            return {\"type\": \"duration\", \"value\": token - self.start_duration}\n",
    "        else:\n",
    "            return {\"type\": \"instrument\", \"value\": token - self.start_instrument}\n",
    "\n",
    "    def quantize_duration(self, duration):\n",
    "        # Quantize duration to one of num_durations bins\n",
    "        # Using log scale to better represent shorter durations\n",
    "        max_duration = 4.0  # Maximum duration in seconds\n",
    "        if duration > max_duration:\n",
    "            duration = max_duration\n",
    "\n",
    "        # Log scale quantization\n",
    "        bin_idx = int(self.num_durations * math.log(1 + duration * 10) / math.log(1 + max_duration * 10))\n",
    "        return min(bin_idx, self.num_durations - 1)\n",
    "\n",
    "    def dequantize_duration(self, bin_idx):\n",
    "        # Convert bin back to duration\n",
    "        max_duration = 4.0\n",
    "        return (math.exp(bin_idx * math.log(1 + max_duration * 10) / self.num_durations) - 1) / 10\n",
    "\n",
    "    def midi_to_sequence(self, midi_file):\n",
    "        \"\"\"Convert MIDI file to token sequence\"\"\"\n",
    "        if isinstance(midi_file, str):\n",
    "            midi_data = pretty_midi.PrettyMIDI(midi_file)\n",
    "        else:\n",
    "            midi_data = midi_file\n",
    "\n",
    "        # Sort all notes by their start time\n",
    "        all_notes = []\n",
    "        for i, instrument in enumerate(midi_data.instruments):\n",
    "            instrument_id = min(i, self.num_instruments - 1)  # Limit to available instrument tokens\n",
    "            for note in instrument.notes:\n",
    "                all_notes.append({\n",
    "                    'start': note.start,\n",
    "                    'end': note.end,\n",
    "                    'pitch': note.pitch,\n",
    "                    'instrument': instrument_id\n",
    "                })\n",
    "\n",
    "        all_notes.sort(key=lambda x: x['start'])\n",
    "\n",
    "        # Convert to token sequence\n",
    "        tokens = [self.sos_token]\n",
    "        for note in all_notes:\n",
    "            duration = note['end'] - note['start']\n",
    "            duration_bin = self.quantize_duration(duration)\n",
    "            note_tokens = self.encode_note(note['pitch'], duration_bin, note['instrument'])\n",
    "            tokens.extend(note_tokens)\n",
    "\n",
    "        tokens.append(self.eos_token)\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def sequence_to_midi(self, tokens, tempo=120):\n",
    "        \"\"\"Convert token sequence back to MIDI\"\"\"\n",
    "        midi_data = pretty_midi.PrettyMIDI(initial_tempo=tempo)\n",
    "        instruments = [pretty_midi.Instrument(program=i) for i in range(self.num_instruments)]\n",
    "\n",
    "        current_time = 0.0\n",
    "        current_instrument = 0\n",
    "        current_pitch = 60\n",
    "\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            token = tokens[i]\n",
    "            if token == self.eos_token:\n",
    "                break\n",
    "\n",
    "            token_info = self.decode_token(token)\n",
    "\n",
    "            if token_info['type'] == 'instrument':\n",
    "                current_instrument = token_info['value']\n",
    "                i += 1\n",
    "            elif token_info['type'] == 'pitch':\n",
    "                current_pitch = token_info['value']\n",
    "\n",
    "                # Look ahead for duration\n",
    "                if i + 1 < len(tokens):\n",
    "                    next_token = tokens[i + 1]\n",
    "                    next_info = self.decode_token(next_token)\n",
    "                    if next_info['type'] == 'duration':\n",
    "                        duration = self.dequantize_duration(next_info['value'])\n",
    "\n",
    "                        # Create a note\n",
    "                        note = pretty_midi.Note(\n",
    "                            velocity=100,\n",
    "                            pitch=current_pitch,\n",
    "                            start=current_time,\n",
    "                            end=current_time + duration\n",
    "                        )\n",
    "\n",
    "                        instruments[current_instrument].notes.append(note)\n",
    "                        current_time += duration\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        i += 1\n",
    "                else:\n",
    "                    i += 1\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "        # Add instruments to MIDI data\n",
    "        for instrument in instruments:\n",
    "            if len(instrument.notes) > 0:\n",
    "                midi_data.instruments.append(instrument)\n",
    "\n",
    "        return midi_data\n",
    "\n",
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, midi_files, processor, max_seq_len):\n",
    "        self.midi_files = midi_files\n",
    "        self.processor = processor\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.midi_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        midi_file = self.midi_files[idx]\n",
    "        try:\n",
    "            # Convert MIDI to token sequence\n",
    "            tokens = self.processor.midi_to_sequence(midi_file)\n",
    "\n",
    "            # Truncate or pad sequence\n",
    "            if len(tokens) > self.max_seq_len:\n",
    "                tokens = tokens[:self.max_seq_len]\n",
    "            else:\n",
    "                tokens = tokens + [PAD_TOKEN] * (self.max_seq_len - len(tokens))\n",
    "\n",
    "            # Convert to tensors\n",
    "            src = torch.tensor(tokens[:-1], dtype=torch.long)\n",
    "            tgt = torch.tensor(tokens[1:], dtype=torch.long)\n",
    "\n",
    "            return src, tgt\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {midi_file}: {e}\")\n",
    "            # Return a simple sequence in case of error\n",
    "            src = torch.tensor([SOS_TOKEN] + [PAD_TOKEN] * (self.max_seq_len - 2), dtype=torch.long)\n",
    "            tgt = torch.tensor([PAD_TOKEN] * (self.max_seq_len - 2) + [EOS_TOKEN], dtype=torch.long)\n",
    "            return src, tgt\n",
    "\n",
    "def train_model(model, train_dataloader, val_dataloader=None, epochs=10, lr=0.0001, device='cuda'):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch_idx, (src, tgt) in enumerate(train_dataloader):\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            src = src.transpose(0, 1)  # Change to (seq_len, batch_size)\n",
    "            tgt_input = tgt.transpose(0, 1)[:-1, :]  # Exclude the last token\n",
    "            tgt_output = tgt.transpose(0, 1)[1:, :]  # Exclude the first token\n",
    "\n",
    "            # Create masks\n",
    "            src_padding_mask = (src == PAD_TOKEN).transpose(0, 1)\n",
    "            tgt_padding_mask = (tgt_input == PAD_TOKEN).transpose(0, 1)\n",
    "\n",
    "            output = model(src, tgt_input)\n",
    "\n",
    "            # Reshape for loss calculation\n",
    "            output = output.view(-1, output.shape[-1])\n",
    "            tgt_output = tgt_output.contiguous().view(-1)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(output, tgt_output)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                print(f'Epoch {epoch+1}, Batch {batch_idx+1}, Loss {loss.item():.4f}')\n",
    "\n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "        print(f'Epoch {epoch+1}, Average loss: {avg_loss:.4f}')\n",
    "\n",
    "        # Validation\n",
    "        if val_dataloader is not None:\n",
    "            val_loss = evaluate(model, val_dataloader, criterion, device)\n",
    "            print(f'Validation loss: {val_loss:.4f}')\n",
    "\n",
    "    return model\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in dataloader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            src = src.transpose(0, 1)\n",
    "            tgt_input = tgt.transpose(0, 1)[:-1, :]\n",
    "            tgt_output = tgt.transpose(0, 1)[1:, :]\n",
    "\n",
    "            output = model(src, tgt_input)\n",
    "\n",
    "            # Reshape for loss calculation\n",
    "            output = output.view(-1, output.shape[-1])\n",
    "            tgt_output = tgt_output.contiguous().view(-1)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(output, tgt_output)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def generate_music(model, seed_midi, processor, max_length=1024, temperature=1.0, device='cuda'):\n",
    "    model.eval()\n",
    "\n",
    "    # Process seed\n",
    "    seed_tokens = processor.midi_to_sequence(seed_midi)\n",
    "    if len(seed_tokens) > max_length // 2:\n",
    "        seed_tokens = seed_tokens[:max_length // 2]\n",
    "\n",
    "    # Convert to tensor\n",
    "    seed_tensor = torch.tensor(seed_tokens).unsqueeze(1).to(device)  # (seq_len, 1)\n",
    "\n",
    "    # Initialize target with SOS token\n",
    "    tgt = torch.tensor([[SOS_TOKEN]]).to(device)  # (1, 1)\n",
    "\n",
    "    # Generate sequence\n",
    "    generated_tokens = [SOS_TOKEN]\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        # Create masks\n",
    "        tgt_mask = model.generate_square_subsequent_mask(tgt.size(0)).to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(seed_tensor, tgt, tgt_mask=tgt_mask)\n",
    "        next_token_logits = output[-1, 0] / temperature\n",
    "        next_token = torch.multinomial(F.softmax(next_token_logits, dim=-1), 1).item()\n",
    "\n",
    "        # Add to sequence\n",
    "        generated_tokens.append(next_token)\n",
    "        next_token_tensor = torch.tensor([[next_token]]).to(device)\n",
    "        tgt = torch.cat([tgt, next_token_tensor], dim=0)\n",
    "\n",
    "        # Stop if EOS token is generated\n",
    "        if next_token == EOS_TOKEN:\n",
    "            break\n",
    "\n",
    "    # Convert tokens back to MIDI\n",
    "    midi_data = processor.sequence_to_midi(generated_tokens)\n",
    "    return midi_data\n",
    "\n",
    "def main():\n",
    "    # Example usage\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Initialize processor\n",
    "    processor = MIDIProcessor()\n",
    "\n",
    "    # Initialize model\n",
    "    model = MusicTransformer(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        d_model=D_MODEL,\n",
    "        n_heads=N_HEADS,\n",
    "        n_layers=N_LAYERS,\n",
    "        d_ff=D_FF,\n",
    "        max_seq_len=MAX_SEQ_LEN,\n",
    "        dropout=DROPOUT\n",
    "    )\n",
    "\n",
    "    # Example: Load MIDI files for training\n",
    "    # This is a placeholder - you would need to provide your own MIDI files\n",
    "    import glob\n",
    "    midi_files = glob.glob('/Users/claude/Dev/bach/data_cache/data/*.mid')\n",
    "\n",
    "    if midi_files:\n",
    "        # Create dataset and dataloader\n",
    "        dataset = MusicDataset(midi_files, processor, MAX_SEQ_LEN)\n",
    "        train_size = int(0.8 * len(dataset))\n",
    "        val_size = len(dataset) - train_size\n",
    "        train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=8)\n",
    "\n",
    "        # Train model\n",
    "        model = train_model(\n",
    "            model=model,\n",
    "            train_dataloader=train_dataloader,\n",
    "            val_dataloader=val_dataloader,\n",
    "            epochs=10,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # Save model\n",
    "        torch.save(model.state_dict(), 'music_transformer.pth')\n",
    "\n",
    "        # Generate music from seed\n",
    "        if midi_files:\n",
    "            seed_midi = pretty_midi.PrettyMIDI(midi_files[0])\n",
    "            generated_midi = generate_music(model, seed_midi, processor, device=device)\n",
    "            generated_midi.write('generated_music.mid')\n",
    "    else:\n",
    "        print(\"No MIDI files found for training\")\n",
    "\n",
    "# Example of how to use the model for inference only\n",
    "def inference_example(model_path, seed_midi_path, output_path):\n",
    "    # Load the trained model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = MusicTransformer(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        d_model=D_MODEL,\n",
    "        n_heads=N_HEADS,\n",
    "        n_layers=N_LAYERS,\n",
    "        d_ff=D_FF,\n",
    "        max_seq_len=MAX_SEQ_LEN,\n",
    "        dropout=DROPOUT\n",
    "    )\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "\n",
    "    # Initialize processor\n",
    "    processor = MIDIProcessor()\n",
    "\n",
    "    # Load seed MIDI\n",
    "    seed_midi = pretty_midi.PrettyMIDI(seed_midi_path)\n",
    "\n",
    "    # Generate music\n",
    "    generated_midi = generate_music(\n",
    "        model=model,\n",
    "        seed_midi=seed_midi,\n",
    "        processor=processor,\n",
    "        max_length=1024,\n",
    "        temperature=1.0,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Save generated music\n",
    "    generated_midi.write(output_path)\n",
    "    print(f\"Generated music saved to {output_path}\")"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T01:22:29.137656Z",
     "start_time": "2025-03-16T01:19:13.489405Z"
    }
   },
   "cell_type": "code",
   "source": "main()",
   "id": "5b7877667899edae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/claude/Dev/bach/env/lib/python3.12/site-packages/pretty_midi/pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing /Users/claude/Dev/bach/data_cache/data/haendel-concertos_grossos_hwv319-330_op06-concerto_grosso_op6_n05_4mov.mid: data byte must be in range 0..127\n",
      "Error processing /Users/claude/Dev/bach/data_cache/data/buxehude-buxethude_buxwv138_prelude.mid: list index out of range\n",
      "Error processing /Users/claude/Dev/bach/data_cache/data/unknown_artist-a_h-clarinet_3.mid: MThd not found. Probably not a MIDI file\n",
      "Epoch 1, Batch 10, Loss 5.2933\n",
      "Error processing /Users/claude/Dev/bach/data_cache/data/unknown_artist-i_o-mappari.mid: MThd not found. Probably not a MIDI file\n",
      "Error processing /Users/claude/Dev/bach/data_cache/data/unknown_artist-p_z-schumann.mid: \n",
      "Error processing /Users/claude/Dev/bach/data_cache/data/unknown_artist-a_h-canon_ind.mid: MThd not found. Probably not a MIDI file\n",
      "Error processing /Users/claude/Dev/bach/data_cache/data/unknown_artist-p_z-tribal_america.mid: \n",
      "Epoch 1, Batch 20, Loss 4.8732\n",
      "Error processing /Users/claude/Dev/bach/data_cache/data/unknown_artist-a_h-gershwin_2.mid: MThd not found. Probably not a MIDI file\n",
      "Error processing /Users/claude/Dev/bach/data_cache/data/unknown_artist-a_h-beet_52_s.mid: no MTrk header at start of track\n",
      "Epoch 1, Batch 30, Loss 4.6885\n",
      "Error processing /Users/claude/Dev/bach/data_cache/data/unknown_artist-p_z-pmp_circ.mid: MThd not found. Probably not a MIDI file\n",
      "Epoch 1, Batch 40, Loss 4.7026\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 398\u001B[39m, in \u001B[36mmain\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    395\u001B[39m val_dataloader = DataLoader(val_dataset, batch_size=\u001B[32m8\u001B[39m)\n\u001B[32m    397\u001B[39m \u001B[38;5;66;03m# Train model\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m398\u001B[39m model = \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    399\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    400\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    401\u001B[39m \u001B[43m    \u001B[49m\u001B[43mval_dataloader\u001B[49m\u001B[43m=\u001B[49m\u001B[43mval_dataloader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    402\u001B[39m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m10\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    403\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice\u001B[49m\n\u001B[32m    404\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    406\u001B[39m \u001B[38;5;66;03m# Save model\u001B[39;00m\n\u001B[32m    407\u001B[39m torch.save(model.state_dict(), \u001B[33m'\u001B[39m\u001B[33mmusic_transformer.pth\u001B[39m\u001B[33m'\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 269\u001B[39m, in \u001B[36mtrain_model\u001B[39m\u001B[34m(model, train_dataloader, val_dataloader, epochs, lr, device)\u001B[39m\n\u001B[32m    266\u001B[39m src_padding_mask = (src == PAD_TOKEN).transpose(\u001B[32m0\u001B[39m, \u001B[32m1\u001B[39m)\n\u001B[32m    267\u001B[39m tgt_padding_mask = (tgt_input == PAD_TOKEN).transpose(\u001B[32m0\u001B[39m, \u001B[32m1\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m269\u001B[39m output = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt_input\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    271\u001B[39m \u001B[38;5;66;03m# Reshape for loss calculation\u001B[39;00m\n\u001B[32m    272\u001B[39m output = output.view(-\u001B[32m1\u001B[39m, output.shape[-\u001B[32m1\u001B[39m])\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Dev/bach/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Dev/bach/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 73\u001B[39m, in \u001B[36mMusicTransformer.forward\u001B[39m\u001B[34m(self, src, tgt, src_mask, tgt_mask)\u001B[39m\n\u001B[32m     70\u001B[39m tgt = \u001B[38;5;28mself\u001B[39m.pos_encoder(tgt)\n\u001B[32m     72\u001B[39m memory = \u001B[38;5;28mself\u001B[39m.transformer_encoder(src, src_mask)\n\u001B[32m---> \u001B[39m\u001B[32m73\u001B[39m output = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtransformer_decoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtgt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmemory\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     74\u001B[39m output = \u001B[38;5;28mself\u001B[39m.fc_out(output)\n\u001B[32m     76\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m output\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Dev/bach/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Dev/bach/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Dev/bach/env/lib/python3.12/site-packages/torch/nn/modules/transformer.py:613\u001B[39m, in \u001B[36mTransformerDecoder.forward\u001B[39m\u001B[34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001B[39m\n\u001B[32m    610\u001B[39m tgt_is_causal = _detect_is_causal_mask(tgt_mask, tgt_is_causal, seq_len)\n\u001B[32m    612\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m mod \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.layers:\n\u001B[32m--> \u001B[39m\u001B[32m613\u001B[39m     output = \u001B[43mmod\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    614\u001B[39m \u001B[43m        \u001B[49m\u001B[43moutput\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    615\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmemory\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    616\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtgt_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtgt_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    617\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmemory_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmemory_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    618\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtgt_key_padding_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtgt_key_padding_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    619\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmemory_key_padding_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmemory_key_padding_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    620\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtgt_is_causal\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtgt_is_causal\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    621\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmemory_is_causal\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmemory_is_causal\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    622\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    624\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.norm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    625\u001B[39m     output = \u001B[38;5;28mself\u001B[39m.norm(output)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Dev/bach/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Dev/bach/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Dev/bach/env/lib/python3.12/site-packages/torch/nn/modules/transformer.py:1108\u001B[39m, in \u001B[36mTransformerDecoderLayer.forward\u001B[39m\u001B[34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001B[39m\n\u001B[32m   1105\u001B[39m     x = x + \u001B[38;5;28mself\u001B[39m._ff_block(\u001B[38;5;28mself\u001B[39m.norm3(x))\n\u001B[32m   1106\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1107\u001B[39m     x = \u001B[38;5;28mself\u001B[39m.norm1(\n\u001B[32m-> \u001B[39m\u001B[32m1108\u001B[39m         x + \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_sa_block\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt_key_padding_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt_is_causal\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1109\u001B[39m     )\n\u001B[32m   1110\u001B[39m     x = \u001B[38;5;28mself\u001B[39m.norm2(\n\u001B[32m   1111\u001B[39m         x\n\u001B[32m   1112\u001B[39m         + \u001B[38;5;28mself\u001B[39m._mha_block(\n\u001B[32m   1113\u001B[39m             x, memory, memory_mask, memory_key_padding_mask, memory_is_causal\n\u001B[32m   1114\u001B[39m         )\n\u001B[32m   1115\u001B[39m     )\n\u001B[32m   1116\u001B[39m     x = \u001B[38;5;28mself\u001B[39m.norm3(x + \u001B[38;5;28mself\u001B[39m._ff_block(x))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Dev/bach/env/lib/python3.12/site-packages/torch/nn/modules/transformer.py:1128\u001B[39m, in \u001B[36mTransformerDecoderLayer._sa_block\u001B[39m\u001B[34m(self, x, attn_mask, key_padding_mask, is_causal)\u001B[39m\n\u001B[32m   1121\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_sa_block\u001B[39m(\n\u001B[32m   1122\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   1123\u001B[39m     x: Tensor,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1126\u001B[39m     is_causal: \u001B[38;5;28mbool\u001B[39m = \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m   1127\u001B[39m ) -> Tensor:\n\u001B[32m-> \u001B[39m\u001B[32m1128\u001B[39m     x = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mself_attn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1129\u001B[39m \u001B[43m        \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1130\u001B[39m \u001B[43m        \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1131\u001B[39m \u001B[43m        \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1132\u001B[39m \u001B[43m        \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1133\u001B[39m \u001B[43m        \u001B[49m\u001B[43mkey_padding_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkey_padding_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1134\u001B[39m \u001B[43m        \u001B[49m\u001B[43mis_causal\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_causal\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1135\u001B[39m \u001B[43m        \u001B[49m\u001B[43mneed_weights\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m   1136\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m[\u001B[32m0\u001B[39m]\n\u001B[32m   1137\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.dropout1(x)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Dev/bach/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Dev/bach/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Dev/bach/env/lib/python3.12/site-packages/torch/nn/modules/activation.py:1373\u001B[39m, in \u001B[36mMultiheadAttention.forward\u001B[39m\u001B[34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001B[39m\n\u001B[32m   1347\u001B[39m     attn_output, attn_output_weights = F.multi_head_attention_forward(\n\u001B[32m   1348\u001B[39m         query,\n\u001B[32m   1349\u001B[39m         key,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1370\u001B[39m         is_causal=is_causal,\n\u001B[32m   1371\u001B[39m     )\n\u001B[32m   1372\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1373\u001B[39m     attn_output, attn_output_weights = \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmulti_head_attention_forward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1374\u001B[39m \u001B[43m        \u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1375\u001B[39m \u001B[43m        \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1376\u001B[39m \u001B[43m        \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1377\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43membed_dim\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1378\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mnum_heads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1379\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43min_proj_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1380\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43min_proj_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1381\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias_k\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1382\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias_v\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1383\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43madd_zero_attn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1384\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdropout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1385\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mout_proj\u001B[49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1386\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mout_proj\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1387\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtraining\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1388\u001B[39m \u001B[43m        \u001B[49m\u001B[43mkey_padding_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkey_padding_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1389\u001B[39m \u001B[43m        \u001B[49m\u001B[43mneed_weights\u001B[49m\u001B[43m=\u001B[49m\u001B[43mneed_weights\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1390\u001B[39m \u001B[43m        \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1391\u001B[39m \u001B[43m        \u001B[49m\u001B[43maverage_attn_weights\u001B[49m\u001B[43m=\u001B[49m\u001B[43maverage_attn_weights\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1392\u001B[39m \u001B[43m        \u001B[49m\u001B[43mis_causal\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_causal\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1393\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1394\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.batch_first \u001B[38;5;129;01mand\u001B[39;00m is_batched:\n\u001B[32m   1395\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m attn_output.transpose(\u001B[32m1\u001B[39m, \u001B[32m0\u001B[39m), attn_output_weights\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Dev/bach/env/lib/python3.12/site-packages/torch/nn/functional.py:6410\u001B[39m, in \u001B[36mmulti_head_attention_forward\u001B[39m\u001B[34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001B[39m\n\u001B[32m   6407\u001B[39m k = k.view(bsz, num_heads, src_len, head_dim)\n\u001B[32m   6408\u001B[39m v = v.view(bsz, num_heads, src_len, head_dim)\n\u001B[32m-> \u001B[39m\u001B[32m6410\u001B[39m attn_output = \u001B[43mscaled_dot_product_attention\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   6411\u001B[39m \u001B[43m    \u001B[49m\u001B[43mq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdropout_p\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_causal\u001B[49m\n\u001B[32m   6412\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   6413\u001B[39m attn_output = (\n\u001B[32m   6414\u001B[39m     attn_output.permute(\u001B[32m2\u001B[39m, \u001B[32m0\u001B[39m, \u001B[32m1\u001B[39m, \u001B[32m3\u001B[39m).contiguous().view(bsz * tgt_len, embed_dim)\n\u001B[32m   6415\u001B[39m )\n\u001B[32m   6417\u001B[39m attn_output = linear(attn_output, out_proj_weight, out_proj_bias)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
