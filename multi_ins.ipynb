{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-16T01:41:32.481143Z",
     "start_time": "2025-03-16T01:41:31.529300Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pretty_midi\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Constants\n",
    "MAX_SEQ_LEN = 512  # Maximum sequence length\n",
    "VOCAB_SIZE = 512   # Size of vocabulary (pitch + duration + instrument + special tokens)\n",
    "D_MODEL = 256      # Embedding dimension\n",
    "N_HEADS = 8        # Number of attention heads\n",
    "N_LAYERS = 6       # Number of transformer layers\n",
    "D_FF = 1024        # Feedforward dimension\n",
    "DROPOUT = 0.1      # Dropout rate\n",
    "\n",
    "# Special tokens\n",
    "PAD_TOKEN = 0\n",
    "SOS_TOKEN = 1\n",
    "EOS_TOKEN = 2\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "class MusicTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, n_layers, d_ff, max_seq_len, dropout):\n",
    "        super(MusicTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_seq_len)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads,\n",
    "                                                  dim_feedforward=d_ff, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=n_layers)\n",
    "        decoder_layers = nn.TransformerDecoderLayer(d_model=d_model, nhead=n_heads,\n",
    "                                                  dim_feedforward=d_ff, dropout=dropout)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layers, num_layers=n_layers)\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        if src_mask is None:\n",
    "            src_mask = self.generate_square_subsequent_mask(src.size(0)).to(src.device)\n",
    "        if tgt_mask is None:\n",
    "            tgt_mask = self.generate_square_subsequent_mask(tgt.size(0)).to(tgt.device)\n",
    "\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "\n",
    "        tgt = self.embedding(tgt) * math.sqrt(self.d_model)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "\n",
    "        memory = self.transformer_encoder(src, src_mask)\n",
    "        output = self.transformer_decoder(tgt, memory, tgt_mask)\n",
    "        output = self.fc_out(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "class MIDIProcessor:\n",
    "    def __init__(self, vocab_size=VOCAB_SIZE):\n",
    "        # Reserved tokens\n",
    "        self.pad_token = PAD_TOKEN\n",
    "        self.sos_token = SOS_TOKEN\n",
    "        self.eos_token = EOS_TOKEN\n",
    "\n",
    "        # Token ranges\n",
    "        self.start_pitch = 10\n",
    "        self.num_pitches = 128\n",
    "        self.start_duration = self.start_pitch + self.num_pitches\n",
    "        self.num_durations = 100  # Quantized durations\n",
    "        self.start_instrument = self.start_duration + self.num_durations\n",
    "        self.num_instruments = 16  # General MIDI has 16 instrument families\n",
    "\n",
    "        assert self.start_instrument + self.num_instruments < vocab_size, \"Vocabulary size too small\"\n",
    "\n",
    "    def encode_note(self, pitch, duration_bin, instrument):\n",
    "        pitch_token = self.start_pitch + pitch\n",
    "        duration_token = self.start_duration + duration_bin\n",
    "        instrument_token = self.start_instrument + instrument\n",
    "        return [instrument_token, pitch_token, duration_token]\n",
    "\n",
    "    def decode_token(self, token):\n",
    "        if token < self.start_pitch:\n",
    "            return {\"type\": \"special\", \"value\": token}\n",
    "        elif token < self.start_duration:\n",
    "            return {\"type\": \"pitch\", \"value\": token - self.start_pitch}\n",
    "        elif token < self.start_instrument:\n",
    "            return {\"type\": \"duration\", \"value\": token - self.start_duration}\n",
    "        else:\n",
    "            return {\"type\": \"instrument\", \"value\": token - self.start_instrument}\n",
    "\n",
    "    def quantize_duration(self, duration):\n",
    "        # Quantize duration to one of num_durations bins\n",
    "        # Using log scale to better represent shorter durations\n",
    "        max_duration = 4.0  # Maximum duration in seconds\n",
    "        if duration > max_duration:\n",
    "            duration = max_duration\n",
    "\n",
    "        # Log scale quantization\n",
    "        bin_idx = int(self.num_durations * math.log(1 + duration * 10) / math.log(1 + max_duration * 10))\n",
    "        return min(bin_idx, self.num_durations - 1)\n",
    "\n",
    "    def dequantize_duration(self, bin_idx):\n",
    "        # Convert bin back to duration\n",
    "        max_duration = 4.0\n",
    "        return (math.exp(bin_idx * math.log(1 + max_duration * 10) / self.num_durations) - 1) / 10\n",
    "\n",
    "    def midi_to_sequence(self, midi_file):\n",
    "        \"\"\"Convert MIDI file to token sequence\"\"\"\n",
    "        if isinstance(midi_file, str):\n",
    "            midi_data = pretty_midi.PrettyMIDI(midi_file)\n",
    "        else:\n",
    "            midi_data = midi_file\n",
    "\n",
    "        # Sort all notes by their start time\n",
    "        all_notes = []\n",
    "        for i, instrument in enumerate(midi_data.instruments):\n",
    "            instrument_id = min(i, self.num_instruments - 1)  # Limit to available instrument tokens\n",
    "            for note in instrument.notes:\n",
    "                all_notes.append({\n",
    "                    'start': note.start,\n",
    "                    'end': note.end,\n",
    "                    'pitch': note.pitch,\n",
    "                    'instrument': instrument_id\n",
    "                })\n",
    "\n",
    "        all_notes.sort(key=lambda x: x['start'])\n",
    "\n",
    "        # Convert to token sequence\n",
    "        tokens = [self.sos_token]\n",
    "        for note in all_notes:\n",
    "            duration = note['end'] - note['start']\n",
    "            duration_bin = self.quantize_duration(duration)\n",
    "            note_tokens = self.encode_note(note['pitch'], duration_bin, note['instrument'])\n",
    "            tokens.extend(note_tokens)\n",
    "\n",
    "        tokens.append(self.eos_token)\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def sequence_to_midi(self, tokens, tempo=120):\n",
    "        \"\"\"Convert token sequence back to MIDI\"\"\"\n",
    "        midi_data = pretty_midi.PrettyMIDI(initial_tempo=tempo)\n",
    "        instruments = [pretty_midi.Instrument(program=i) for i in range(self.num_instruments)]\n",
    "\n",
    "        current_time = 0.0\n",
    "        current_instrument = 0\n",
    "        current_pitch = 60\n",
    "\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            token = tokens[i]\n",
    "            if token == self.eos_token:\n",
    "                break\n",
    "\n",
    "            token_info = self.decode_token(token)\n",
    "\n",
    "            if token_info['type'] == 'instrument':\n",
    "                current_instrument = token_info['value']\n",
    "                i += 1\n",
    "            elif token_info['type'] == 'pitch':\n",
    "                current_pitch = token_info['value']\n",
    "\n",
    "                # Look ahead for duration\n",
    "                if i + 1 < len(tokens):\n",
    "                    next_token = tokens[i + 1]\n",
    "                    next_info = self.decode_token(next_token)\n",
    "                    if next_info['type'] == 'duration':\n",
    "                        duration = self.dequantize_duration(next_info['value'])\n",
    "\n",
    "                        # Create a note\n",
    "                        note = pretty_midi.Note(\n",
    "                            velocity=100,\n",
    "                            pitch=current_pitch,\n",
    "                            start=current_time,\n",
    "                            end=current_time + duration\n",
    "                        )\n",
    "\n",
    "                        instruments[current_instrument].notes.append(note)\n",
    "                        current_time += duration\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        i += 1\n",
    "                else:\n",
    "                    i += 1\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "        # Add instruments to MIDI data\n",
    "        for instrument in instruments:\n",
    "            if len(instrument.notes) > 0:\n",
    "                midi_data.instruments.append(instrument)\n",
    "\n",
    "        return midi_data\n",
    "\n",
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, midi_files, processor, max_seq_len):\n",
    "        self.midi_files = midi_files\n",
    "        self.processor = processor\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.midi_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        midi_file = self.midi_files[idx]\n",
    "        try:\n",
    "            # Convert MIDI to token sequence\n",
    "            tokens = self.processor.midi_to_sequence(midi_file)\n",
    "\n",
    "            # Truncate or pad sequence\n",
    "            if len(tokens) > self.max_seq_len:\n",
    "                tokens = tokens[:self.max_seq_len]\n",
    "            else:\n",
    "                tokens = tokens + [PAD_TOKEN] * (self.max_seq_len - len(tokens))\n",
    "\n",
    "            # Convert to tensors\n",
    "            src = torch.tensor(tokens[:-1], dtype=torch.long)\n",
    "            tgt = torch.tensor(tokens[1:], dtype=torch.long)\n",
    "\n",
    "            return src, tgt\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {midi_file}: {e}\")\n",
    "            # Return a simple sequence in case of error\n",
    "            src = torch.tensor([SOS_TOKEN] + [PAD_TOKEN] * (self.max_seq_len - 2), dtype=torch.long)\n",
    "            tgt = torch.tensor([PAD_TOKEN] * (self.max_seq_len - 2) + [EOS_TOKEN], dtype=torch.long)\n",
    "            return src, tgt\n",
    "\n",
    "def train_model(model, train_dataloader, val_dataloader=None, epochs=10, lr=0.0001, device='cuda'):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch_idx, (src, tgt) in enumerate(train_dataloader):\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            src = src.transpose(0, 1)  # Change to (seq_len, batch_size)\n",
    "            tgt_input = tgt.transpose(0, 1)[:-1, :]  # Exclude the last token\n",
    "            tgt_output = tgt.transpose(0, 1)[1:, :]  # Exclude the first token\n",
    "\n",
    "            # Create masks\n",
    "            src_padding_mask = (src == PAD_TOKEN).transpose(0, 1)\n",
    "            tgt_padding_mask = (tgt_input == PAD_TOKEN).transpose(0, 1)\n",
    "\n",
    "            output = model(src, tgt_input)\n",
    "\n",
    "            # Reshape for loss calculation\n",
    "            output = output.view(-1, output.shape[-1])\n",
    "            tgt_output = tgt_output.contiguous().view(-1)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(output, tgt_output)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                print(f'Epoch {epoch+1}, Batch {batch_idx+1}, Loss {loss.item():.4f}')\n",
    "\n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "        print(f'Epoch {epoch+1}, Average loss: {avg_loss:.4f}')\n",
    "\n",
    "        # Validation\n",
    "        if val_dataloader is not None:\n",
    "            val_loss = evaluate(model, val_dataloader, criterion, device)\n",
    "            print(f'Validation loss: {val_loss:.4f}')\n",
    "\n",
    "    return model\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in dataloader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            src = src.transpose(0, 1)\n",
    "            tgt_input = tgt.transpose(0, 1)[:-1, :]\n",
    "            tgt_output = tgt.transpose(0, 1)[1:, :]\n",
    "\n",
    "            output = model(src, tgt_input)\n",
    "\n",
    "            # Reshape for loss calculation\n",
    "            output = output.view(-1, output.shape[-1])\n",
    "            tgt_output = tgt_output.contiguous().view(-1)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(output, tgt_output)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def generate_music(model, seed_midi, processor, max_length=1024, temperature=1.0, device='cuda'):\n",
    "    model.eval()\n",
    "\n",
    "    # Process seed\n",
    "    seed_tokens = processor.midi_to_sequence(seed_midi)\n",
    "    if len(seed_tokens) > max_length // 2:\n",
    "        seed_tokens = seed_tokens[:max_length // 2]\n",
    "\n",
    "    # Convert to tensor\n",
    "    seed_tensor = torch.tensor(seed_tokens).unsqueeze(1).to(device)  # (seq_len, 1)\n",
    "\n",
    "    # Initialize target with SOS token\n",
    "    tgt = torch.tensor([[SOS_TOKEN]]).to(device)  # (1, 1)\n",
    "\n",
    "    # Generate sequence\n",
    "    generated_tokens = [SOS_TOKEN]\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        # Create masks\n",
    "        tgt_mask = model.generate_square_subsequent_mask(tgt.size(0)).to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(seed_tensor, tgt, tgt_mask=tgt_mask)\n",
    "        next_token_logits = output[-1, 0] / temperature\n",
    "        next_token = torch.multinomial(F.softmax(next_token_logits, dim=-1), 1).item()\n",
    "\n",
    "        # Add to sequence\n",
    "        generated_tokens.append(next_token)\n",
    "        next_token_tensor = torch.tensor([[next_token]]).to(device)\n",
    "        tgt = torch.cat([tgt, next_token_tensor], dim=0)\n",
    "\n",
    "        # Stop if EOS token is generated\n",
    "        if next_token == EOS_TOKEN:\n",
    "            break\n",
    "\n",
    "    # Convert tokens back to MIDI\n",
    "    midi_data = processor.sequence_to_midi(generated_tokens)\n",
    "    return midi_data\n",
    "\n",
    "def main():\n",
    "    # Example usage\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Initialize processor\n",
    "    processor = MIDIProcessor()\n",
    "\n",
    "    # Initialize model\n",
    "    model = MusicTransformer(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        d_model=D_MODEL,\n",
    "        n_heads=N_HEADS,\n",
    "        n_layers=N_LAYERS,\n",
    "        d_ff=D_FF,\n",
    "        max_seq_len=MAX_SEQ_LEN,\n",
    "        dropout=DROPOUT\n",
    "    )\n",
    "\n",
    "    # Example: Load MIDI files for training\n",
    "    # This is a placeholder - you would need to provide your own MIDI files\n",
    "    import glob\n",
    "    midi_files = glob.glob('D:\\\\Dev\\\\repo\\\\bach\\\\data_cache\\\\data\\\\*.mid')\n",
    "\n",
    "    if midi_files:\n",
    "        # Create dataset and dataloader\n",
    "        dataset = MusicDataset(midi_files, processor, MAX_SEQ_LEN)\n",
    "        train_size = int(0.8 * len(dataset))\n",
    "        val_size = len(dataset) - train_size\n",
    "        train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=128)\n",
    "\n",
    "        # Train model\n",
    "        model = train_model(\n",
    "            model=model,\n",
    "            train_dataloader=train_dataloader,\n",
    "            val_dataloader=val_dataloader,\n",
    "            epochs=10,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # Save model\n",
    "        torch.save(model.state_dict(), 'music_transformer.pth')\n",
    "\n",
    "        # Generate music from seed\n",
    "        if midi_files:\n",
    "            seed_midi = pretty_midi.PrettyMIDI(midi_files[0])\n",
    "            generated_midi = generate_music(model, seed_midi, processor, device=device)\n",
    "            generated_midi.write('generated_music.mid')\n",
    "    else:\n",
    "        print(\"No MIDI files found for training\")\n",
    "\n",
    "# Example of how to use the model for inference only\n",
    "def inference_example(model_path, seed_midi_path, output_path):\n",
    "    # Load the trained model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = MusicTransformer(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        d_model=D_MODEL,\n",
    "        n_heads=N_HEADS,\n",
    "        n_layers=N_LAYERS,\n",
    "        d_ff=D_FF,\n",
    "        max_seq_len=MAX_SEQ_LEN,\n",
    "        dropout=DROPOUT\n",
    "    )\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "\n",
    "    # Initialize processor\n",
    "    processor = MIDIProcessor()\n",
    "\n",
    "    # Load seed MIDI\n",
    "    seed_midi = pretty_midi.PrettyMIDI(seed_midi_path)\n",
    "\n",
    "    # Generate music\n",
    "    generated_midi = generate_music(\n",
    "        model=model,\n",
    "        seed_midi=seed_midi,\n",
    "        processor=processor,\n",
    "        max_length=1024,\n",
    "        temperature=1.0,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Save generated music\n",
    "    generated_midi.write(output_path)\n",
    "    print(f\"Generated music saved to {output_path}\")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T01:43:05.737263500Z",
     "start_time": "2025-03-16T01:41:32.486146Z"
    }
   },
   "cell_type": "code",
   "source": "main()",
   "id": "5b7877667899edae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Dev\\repo\\bach\\env\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
